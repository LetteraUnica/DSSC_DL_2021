{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brave-poetry",
   "metadata": {},
   "source": [
    "### Deep Learning Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-aurora",
   "metadata": {},
   "source": [
    "1) Taking inspiration from the notebook `01-intro-to-pt.ipynb`, build a class for the Multilayer Perceptron (MLP) whose scheme is drawn in the last figure of the notebook. As written there, no layer should have bias units and the activation for each hidden layer should be the Rectified Linear Unit (ReLU) function, also called ramp function. The activation leading to the output layer, instead, should be the softmax function, which prof. Ansuini explained during the last lecture. You can find some notions on it also on the notebook.\n",
    "\n",
    "2) After having defined the class, create an instance of it and print a summary using a method of your choice.\n",
    "\n",
    "3) Provide detailed calculations (layer-by-layer) on the exact number of parameters in the network.  \n",
    "    3.1)Provide the same calculation in the case that the bias units are present in all layers (except input).\n",
    "\n",
    "4) For each layer within the MLP, calculate the L2 norm and L1 norm of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statistical-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "relative-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(5, 11, bias=False)\n",
    "        self.layer2 = nn.Linear(11, 16, bias=False)\n",
    "        self.layer3 = nn.Linear(16, 13, bias=False)\n",
    "        self.layer4 = nn.Linear(13, 8, bias=False)\n",
    "        self.output = nn.Linear(8, 4, bias=False)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        relu = nn.functional.relu\n",
    "        softmax = nn.functional.softmax\n",
    "        \n",
    "        out = relu(self.layer1(X))\n",
    "        out = relu(self.layer2(out))\n",
    "        out = relu(self.layer3(out))\n",
    "        out = relu(self.layer4(out))\n",
    "        out = softmax(self.output(out), dim=1)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becoming-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            55\n",
      "├─Linear: 1-2                            176\n",
      "├─Linear: 1-3                            208\n",
      "├─Linear: 1-4                            104\n",
      "├─Linear: 1-5                            32\n",
      "=================================================================\n",
      "Total params: 575\n",
      "Trainable params: 575\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Linear: 1-1                            55\n",
       "├─Linear: 1-2                            176\n",
       "├─Linear: 1-3                            208\n",
       "├─Linear: 1-4                            104\n",
       "├─Linear: 1-5                            32\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2)\n",
    "\n",
    "# Instantiate and test\n",
    "mlp = MLP()\n",
    "X = torch.randn((3, 5))\n",
    "mlp(X)\n",
    "\n",
    "# Summary of the model\n",
    "summary(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-liquid",
   "metadata": {},
   "source": [
    "3) In fully connected layers each neuron of layer $i$ is connected to each neuron of layer $i+1$, so if we define $n_i$ as the number of neurons of layer $i$ each neuron of the this layer will have $n_{i+1}$ connections and since we have $n_i$ neurons this will result in a $n_i \\cdot n_{i+1}$ total number of connections and since each connections represents a parameter we will have $n_i \\cdot n_{i+1}$ parameters. In our network the number of parameters will be:  \n",
    "* input to layer1: 5*11 = 55\n",
    "* layer1 to layer2: 11*16 = 176\n",
    "* layer2 to layer3: 16*13 = 208\n",
    "* layer3 to layer4: 13*8 = 104\n",
    "* layer4 to output: 8*4 = 32\n",
    "\n",
    "Which corresponds to the summary of our model\n",
    "\n",
    "3.1) In case we have a bias the number of parameters will increase, in particular we will have one additional parameter for each neuron in the $n_{i+1}$ layer, which corresponds to $n_i \\cdot n_{i+1} + n_{i+1} = (n_i+1) \\cdot n_{i+1}$ parameters, we can view this as adding an additional neuron in the $n_i$ layer with value 1. In our network this will correspond to:\n",
    "\n",
    "* input to layer1: (5+1)*11 = 66\n",
    "* layer1 to layer2: (11+1)*16 = 192\n",
    "* layer2 to layer3: (16+1)*13 = 221\n",
    "* layer3 to layer4: (13+1)*8 = 112\n",
    "* layer4 to output: (8+1)*4 = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.weight L2 norm: tensor(1.9684) L1 norm: tensor(12.6293)\n",
      "layer2.weight L2 norm: tensor(2.4287) L1 norm: tensor(28.1345)\n",
      "layer3.weight L2 norm: tensor(2.0713) L1 norm: tensor(25.6020)\n",
      "layer4.weight L2 norm: tensor(1.5003) L1 norm: tensor(13.1404)\n",
      "output.weight L2 norm: tensor(1.0281) L1 norm: tensor(4.8594)\n"
     ]
    }
   ],
   "source": [
    "# 4)\n",
    "\n",
    "for param_name, param in mlp.state_dict().items():\n",
    "    print(param_name, \"L2 norm:\", torch.norm(param, 2), \"L1 norm:\", torch.norm(param, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
