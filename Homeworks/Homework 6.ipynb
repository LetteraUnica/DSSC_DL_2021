{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8487daab",
   "metadata": {},
   "source": [
    "### Deep Learning Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a6b24",
   "metadata": {},
   "source": [
    "Taking inspiration from the last 2 pictures within the notebook (07-convnets.ipynb), implement a U-Net-style CNN with the following specs:\n",
    "\n",
    "1. All convolutions must use a 3 x 3 kernel and leave the spatial dimensions (i.e. height, width) of the input untouched.\n",
    "2. Downsampling in the contracting part is performed via maxpooling with a 2 x 2 kernel and stride of 2.\n",
    "3. Upsampling is operated by a deconvolution with a 2 x 2 kernel and stride of 2. The PyTorch module that implements the deconvolution is `nn.ConvTranspose2d`\n",
    "4. The final layer of the expanding part has only 1 channel \n",
    "\n",
    "* between how many classes are we discriminating?\n",
    "\n",
    "Create a network class with (at least) a `__init__` and a `forward` method. Please resort to additional structures (e.g., `nn.Module`s, private methods...) if you believe it helps readability of your code.\n",
    "\n",
    "Test, at least with random data, that the network is doing the correct tensor operations and that the output has the correct shape (e.g., use `assert`s in your code to see if the byproduct is of the expected shape).\n",
    "\n",
    "Note: the overall organization of your work can greatly improve readability and understanding of your code by others. Please consider preparing your notebook in an organized fashion so that we can better understand (and correct) your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4d1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pylab as pl\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacfa60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_block(nn.Module):\n",
    "    \"\"\"Implements a VGG layer with kernel size 3 and padding 1\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_layers=2, maxpool=False, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(activation())\n",
    "        for i in range(num_layers-1):\n",
    "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(activation())\n",
    "        \n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "             \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "    \n",
    "class upsampling_block(nn.Module):\n",
    "    \"\"\"Implements the upsampling block of the U-net, basically it's a VGG_block\n",
    "    with in_channels=in_channels and out_channels=mid_channels followed by a \n",
    "    deconvolution operation that doubles the size of the input image\"\"\"\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, num_mid_layers=2, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            VGG_block(in_channels, mid_channels, num_layers=num_mid_layers, activation=activation),\n",
    "            nn.ConvTranspose2d(mid_channels, out_channels, kernel_size=2, stride=2),\n",
    "            activation()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "    \n",
    "class U_net(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a U-net, this architecture can be trained without changing anything\n",
    "    on non-square and non-power of two images, however the results can be worse\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels=3, depth=4, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Downsampling layers\n",
    "        downsampling_layers = []\n",
    "        in_channels, out_channels = channels, 64\n",
    "        for i in range(depth):\n",
    "            downsampling_layers.append(VGG_block(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        \n",
    "        self.downsampling_layers = nn.Sequential(*downsampling_layers)\n",
    "        # Ceil mode is required if I have uneven images, otherwise it's the same\n",
    "        self.maxpool = nn.MaxPool2d(2, ceil_mode=True)\n",
    "        \n",
    "        # \"Deepest\" layer\n",
    "        mid_channels = out_channels\n",
    "        out_channels = in_channels\n",
    "        self.deep_layer = upsampling_block(in_channels, mid_channels, out_channels)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        upsampling_layers = []\n",
    "        in_channels, mid_channels, out_channels = in_channels*2, mid_channels//2, out_channels//2\n",
    "        for i in range(depth-1):\n",
    "            upsampling_layers.append(upsampling_block(in_channels, mid_channels, out_channels))\n",
    "            in_channels = in_channels//2\n",
    "            mid_channels = mid_channels//2\n",
    "            out_channels = out_channels//2\n",
    "            \n",
    "        self.upsampling_layers = nn.Sequential(*upsampling_layers)\n",
    "        \n",
    "        # Classifier or last layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            VGG_block(in_channels, mid_channels),\n",
    "            nn.Conv2d(mid_channels, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def center_crop(self, images, size_x, size_y):\n",
    "        \"\"\"Rectangle center crop of a set of images\"\"\"\n",
    "        \n",
    "        # If the crop is bigger or equal to the images do nothing\n",
    "        if size_x>=images.size()[2] and size_y>=images.size()[3]:\n",
    "            return images\n",
    "        \n",
    "        # Otherwise perform the cropping\n",
    "        center_x = images.size()[2] // 2\n",
    "        center_y = images.size()[3] // 2\n",
    "        bottom_left = [center_x - size_x//2, center_y - size_y//2]\n",
    "        top_right = [center_x + (size_x+1)//2, center_y + (size_y+1)//2]\n",
    "        return images[:, :, bottom_left[0]: top_right[0], bottom_left[1]: top_right[1]]\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        skips = [] # Holds the skip connections\n",
    "        out = X\n",
    "        \n",
    "        # Downsampling phase\n",
    "        for layer in self.downsampling_layers:\n",
    "            out = layer(out)\n",
    "            skips.append(out)\n",
    "            out = self.maxpool(out)\n",
    "        \n",
    "        # Deepest layer\n",
    "        out = self.deep_layer(out)\n",
    "        \n",
    "        # Upsampling phase\n",
    "        for i, layer in enumerate(self.upsampling_layers, start=1):\n",
    "            # The cropping is done only if the downsampling phase has uneven image sizes\n",
    "            # In that case in the upsampling the resulting image will be 1 pixel\n",
    "            # wider and I need to crop it, notice that this doesn't happen for\n",
    "            # power-of-two images and cropping does nothing\n",
    "            out = self.center_crop(out, *skips[-i].size()[2:])\n",
    "            out = torch.cat((skips[-i], out), dim=1) # Concatenate the previous output\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = self.center_crop(out, *skips[0].size()[2:])\n",
    "        out = torch.cat((skips[0], out), dim=1)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efb212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    channels = randint(1,10)\n",
    "    net = U_net(channels=channels, depth=randint(1,10), num_classes=randint(2, 20))\n",
    "    images = torch.randn((randint(1,10), channels, randint(1, 200), randint(1, 200)))\n",
    "    print(net(images).size())\n",
    "\n",
    "\n",
    "print(net(images).size())\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12f6549e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-5b1e6a21bba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m209\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m210\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-84-5b1e6a21bba2>\u001b[0m in \u001b[0;36mcenter_crop\u001b[0;34m(self, x, size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcenter_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcenter_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "    \n",
    "    def center_crop(self, x, size):\n",
    "        if(size==x.size()[-1]):\n",
    "            return x\n",
    "        center_x = x.size()[2] // 2\n",
    "        center_y = x.size()[3] // 2\n",
    "        center = [center_x, center_y]\n",
    "        return x[:, :, center[0] - size//2: center[0] + (size+1)//2, center[1] - size//2: center[1] + (size+1)//2]\n",
    "    \n",
    "center_crop(images, 209, 210).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
