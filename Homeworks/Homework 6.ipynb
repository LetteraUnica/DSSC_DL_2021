{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1e1bec",
   "metadata": {},
   "source": [
    "### Deep Learning Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78697aa7",
   "metadata": {},
   "source": [
    "Taking inspiration from the last 2 pictures within the notebook (07-convnets.ipynb), implement a U-Net-style CNN with the following specs:\n",
    "\n",
    "1. All convolutions must use a 3 x 3 kernel and leave the spatial dimensions (i.e. height, width) of the input untouched.\n",
    "2. Downsampling in the contracting part is performed via maxpooling with a 2 x 2 kernel and stride of 2.\n",
    "3. Upsampling is operated by a deconvolution with a 2 x 2 kernel and stride of 2. The PyTorch module that implements the deconvolution is `nn.ConvTranspose2d`\n",
    "4. The final layer of the expanding part has only 1 channel \n",
    "\n",
    "* between how many classes are we discriminating?\n",
    "\n",
    "Create a network class with (at least) a `__init__` and a `forward` method. Please resort to additional structures (e.g., `nn.Module`s, private methods...) if you believe it helps readability of your code.\n",
    "\n",
    "Test, at least with random data, that the network is doing the correct tensor operations and that the output has the correct shape (e.g., use `assert`s in your code to see if the byproduct is of the expected shape).\n",
    "\n",
    "Note: the overall organization of your work can greatly improve readability and understanding of your code by others. Please consider preparing your notebook in an organized fashion so that we can better understand (and correct) your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0609b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pylab as pl\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682f9a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.2921, -0.1395, -0.2395,  0.0859, -0.1852, -0.0917, -0.1712, -0.1405,\n",
      "         0.2245,  0.1828], grad_fn=<AddBackward0>)]\n",
      "[tensor([-0.2921, -0.1395, -0.2395,  0.0859, -0.1852, -0.0917, -0.1712, -0.1405,\n",
      "         0.2245,  0.1828], grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((10, ))\n",
    "a = []\n",
    "layer = nn.Linear(10, 10)\n",
    "out = layer(x)\n",
    "a.append(out)\n",
    "\n",
    "print(a)\n",
    "out = layer(out)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea9a2ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2387, -0.2231, -0.2372,  0.1754, -0.2415, -0.0832, -0.1299, -0.2551,\n",
       "         0.3153,  0.1280], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8dd9d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=2, maxpool=False, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(activation())\n",
    "        for i in range(num_layers-1):\n",
    "            layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(activation())\n",
    "        \n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "             \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "    \n",
    "class upsampling_block(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, num_mid_layers=2, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            VGG_block(in_channels, mid_channels, num_layers=num_mid_layers, activation=activation),\n",
    "            nn.ConvTranspose2d(mid_channels, out_channels, kernel_size=2, stride=2),\n",
    "            activation()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "class U_net(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Unet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h=572, w=572, channels=3, depth=4, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Downsampling layers\n",
    "        downsampling_layers = []\n",
    "        in_channels, out_channels = channels, 64\n",
    "        for i in range(depth):\n",
    "            downsampling_layers.append(VGG_block(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        \n",
    "        self.downsampling_layers = nn.Sequential(*downsampling_layers)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Deepest layer\n",
    "        mid_channels = out_channels\n",
    "        out_channels = in_channels\n",
    "        self.deep_layer = upsampling_block(in_channels, mid_channels, out_channels)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        upsampling_layers = []\n",
    "        in_channels, mid_channels, out_channels = mid_channels, out_channels, out_channels//2\n",
    "        for i in range(depth-1):\n",
    "            upsampling_layers.append(upsampling_block(in_channels, mid_channels, out_channels))\n",
    "            in_channels = in_channels//2\n",
    "            mid_channels = mid_channels//2\n",
    "            out_channels = out_channels//2\n",
    "            \n",
    "        self.upsampling_layers = nn.Sequential(*upsampling_layers)\n",
    "        \n",
    "        # Clasifier or last layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            VGG_block(in_channels, mid_channels),\n",
    "            nn.Conv2d(mid_channels, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):        \n",
    "        skips = []\n",
    "        out = X\n",
    "        print(out.size())\n",
    "        for layer in self.downsampling_layers:\n",
    "            out = layer(out)\n",
    "            skips.append(out)\n",
    "            out = self.maxpool(out)\n",
    "            print(out.size())\n",
    "        \n",
    "        out = self.deep_layer(out)\n",
    "            \n",
    "        for i, layer in enumerate(self.upsampling_layers, start=1):\n",
    "            if skips[-i].size()[-1] % 2 != 0:\n",
    "                skips[-i] = skips[-i][:, :, 0:-1, 0:-1]\n",
    "            print(out.size(), skips[-i].size())\n",
    "            out = torch.cat((skips[-i], out), dim=1)\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = torch.cat((skips[0], out), dim=1)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "23507438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "210/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5e178f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 210, 210])\n",
      "torch.Size([10, 64, 105, 105])\n",
      "torch.Size([10, 128, 52, 52])\n",
      "torch.Size([10, 256, 26, 26])\n",
      "torch.Size([10, 512, 13, 13])\n",
      "torch.Size([10, 512, 26, 26]) torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 256, 52, 52]) torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 128, 104, 104]) torch.Size([10, 128, 104, 104])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 210 and 208 in dimension 2 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-3fcb8ba77b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-b6f13c8246c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 210 and 208 in dimension 2 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "images = torch.randn((10, 3, 210, 210))\n",
    "net = U_net()\n",
    "\n",
    "print(net(images).size())\n",
    "\n",
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
