{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the implementation contained within the notebook `05-pruning.ipynb`, extend the  `magnitude_pruning` function to allow for incremental (iterative) pruning. In the current case, if you try pruning one more time, you'll notice that it will not work as there's no way to communicate to the future calls of `magnitude_pruning` to ignore the parameters which have already been pruned. Find a way to enhance the routine s.t. it can effectively prune networks in a sequential fashion (i.e., if we passed an MLP already pruned of 20% of its parameters, we want to prune *another* 20% of parameters).\n",
    "\n",
    "Hint: use the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pylab as pl\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a neural network to train on MNIST \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(784, 384),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(384),\n",
    "            torch.nn.Linear(384, 384),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(384),\n",
    "            torch.nn.Linear(384, 384),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(384),\n",
    "            torch.nn.Linear(384, 10),\n",
    "            torch.nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):        \n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_pruning(model, pruning_rate, mask=None, layers_to_prune=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Implements magnitude pruning on a model, the function returns a ma\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Model to which apply the pruning\n",
    "    \n",
    "    pruning_rate: float\n",
    "        Rate of weights that will be set to 0\n",
    "    \n",
    "    mask: list of Tensors, Optional\n",
    "        Mask of already pruned weights, default: None\n",
    "    \n",
    "    layers_to_prune: list of strings, Optional\n",
    "        Names of the layers to prune, useful to avoid pruning batchnorm layers,\n",
    "        default: None\n",
    "    \n",
    "    verbose: bool, Optional\n",
    "        Whether to print pruning statistics, mainly for testing purposes\n",
    "        default: False\n",
    "    Returns\n",
    "    -------\n",
    "    mask: list of Tensors\n",
    "        List with length the number of layers and for each entry a tensor\n",
    "        of size of the corresponding layer filled with 0 or 1 that tells \n",
    "        if the weight is removed (0) or not (1), the mask is useful to retrain\n",
    "        the model afterwards or reapply pruning a second time\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    \n",
    "    # Zero out the parameters that are already pruned\n",
    "    # Note: This step ensures to prune new parameters even when\n",
    "    # the previously pruned parameters are different from zero\n",
    "    if mask is not None and len(mask) > 0:\n",
    "        params_to_prune = [pars[1]*m for pars, m in zip(model.named_parameters(), mask) if any([l in pars[0] for l in layers_to_prune])]\n",
    "    else:\n",
    "        params_to_prune = [pars[1] for pars in model.named_parameters() if any([l in pars[0] for l in layers_to_prune])]\n",
    "    \n",
    "    flat = torch.cat([pars.abs().flatten() for pars in params_to_prune], dim=0)\n",
    "    \n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "\n",
    "    # 3. obtain the threshold\n",
    "    n_params = flat.size()[0]\n",
    "    # Number of already pruned weights\n",
    "    already_pruned = 0 if mask is None else sum([torch.sum((m-1).abs()).item() for m in mask])\n",
    "    position = int(pruning_rate * (n_params - already_pruned) + already_pruned)\n",
    "    thresh = flat[position]\n",
    "    \n",
    "    if verbose:\n",
    "        s = f\"Total number of parameters: {n_params}\\n\"\n",
    "        s += f\"parameters already pruned: {int(already_pruned)}\\n\"\n",
    "        s += f\"parameters pruned this run: {int(position-already_pruned)}\\n\"\n",
    "        s += f\"pruning rate: {(position-already_pruned) / (n_params-already_pruned)}\\n\"\n",
    "        print(s)\n",
    "\n",
    "    # reset the mask\n",
    "    mask = []\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask &\n",
    "    # 6. obtain the new structure of parameters\n",
    "    '''\n",
    "    I do this process with a for cycle instead of a list comprehension for clarity\n",
    "    * if the layer is a layer to prune → populate the mask with 1s and 0s\n",
    "    * otherwise → just populate the mask with ones\n",
    "    By doing so, I can immediately apply the mask to the model as well...\n",
    "    '''\n",
    "    for pars in model.named_parameters():\n",
    "        # Pruned layers\n",
    "        if any([l in pars[0] for l in layers_to_prune]):\n",
    "            m = torch.where(pars[1].abs() >= thresh, 1, 0)\n",
    "            mask.append(m)\n",
    "            pars[1].data *= m\n",
    "        # Unpruned layers\n",
    "        else:\n",
    "            mask.append(torch.ones_like(pars[1]))\n",
    "\n",
    "    # 7. what do we need to return?\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One shot pruning:\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 0\n",
      "parameters pruned this run: 120194\n",
      "pruning rate: 0.2\n",
      "\n",
      "\n",
      "Recursive pruning:\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 0\n",
      "parameters pruned this run: 120194\n",
      "pruning rate: 0.2\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 120194\n",
      "parameters pruned this run: 96155\n",
      "pruning rate: 0.19999958400585718\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 216349\n",
      "parameters pruned this run: 76924\n",
      "pruning rate: 0.19999948000759188\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 293273\n",
      "parameters pruned this run: 61539\n",
      "pruning rate: 0.1999987000198247\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 354812\n",
      "parameters pruned this run: 49231\n",
      "pruning rate: 0.19999756254113213\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 404043\n",
      "parameters pruned this run: 39385\n",
      "pruning rate: 0.1999979687904655\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 443428\n",
      "parameters pruned this run: 31508\n",
      "pruning rate: 0.19999746099452845\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 474935\n",
      "parameters pruned this run: 25207\n",
      "pruning rate: 0.2\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 500142\n",
      "parameters pruned this run: 20165\n",
      "pruning rate: 0.1999940492720276\n",
      "\n",
      "Total number of parameters: 600970\n",
      "parameters already pruned: 520307\n",
      "parameters pruned this run: 16132\n",
      "pruning rate: 0.19999256164536405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "\n",
    "# Test one shot pruning\n",
    "print(\"One shot pruning:\\n\")\n",
    "_ = magnitude_pruning(net, 0.2, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], verbose=True)\n",
    "\n",
    "# Test recursive pruning\n",
    "print(\"\\nRecursive pruning:\\n\")\n",
    "mask = []\n",
    "for i in range(10):\n",
    "    mask = magnitude_pruning(net, 0.2, mask, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
