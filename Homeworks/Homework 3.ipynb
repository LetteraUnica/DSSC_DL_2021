{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liberal-intellectual",
   "metadata": {},
   "source": [
    "### Deep Learning Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-tracy",
   "metadata": {},
   "source": [
    "Note: * indicates a non-compulsory extra exercise. We won't penalize you if your notebooks won't contain solutions to these.\n",
    "\n",
    "As for previous labs, please provide your solutions in a Jupyter Notebook, trying to interleave code cells with markdown cells to explain what you're doing (if not trivial).\n",
    "\n",
    "1. Implement L1 norm regularization as a custom loss function\n",
    "2. The third-to-last paragraph in the notebook is concerning early stopping, an \"old\" regularization technique which involves the stopping of training earlier than the number of epochs would suggest. Read the paragraph and download the paper from Prechelt et al.\n",
    "\n",
    "    1. Implement early stopping in the E_{opt} specification\n",
    "    2. \\*Implement early stopping in one of the additional specifications\n",
    "\n",
    "3.  \\*(from Lab 2) We have seen how to implement the Quadratic Loss for multinomial classification problems. Read the paper from Demirkaya et al. (in which the Quadratic Loss is introduced along with its issues) and try implementing Correct Class Quadratic Loss (CCQL) in PyTorch as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pylab as pl\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "frozen-irrigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7282, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Implement L1 norm regularization as a custom loss function\n",
    "\n",
    "class L1_loss():\n",
    "    \"\"\"\n",
    "    Implements the L1 norm regularization loss function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L1_coef: float, optional\n",
    "        L1 regularization parameter, the higher the more regularization is applied  \n",
    "        Default: 1e-4\n",
    "    \n",
    "    reduction: string, optional\n",
    "        Specifies the reduction to apply to the output:\n",
    "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "        ``'mean'``: the sum of the output will be divided by the number of\n",
    "        elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, L1_coef: float = 1e-4, reduction: str = 'mean') -> None:\n",
    "        self.reduction = reduction\n",
    "        self.l1 = L1_reg\n",
    "    \n",
    "    \"\"\"\n",
    "    Implements the L1 norm regularization loss function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Model to which apply the regularization, must be an instance of nn.Module\n",
    "    \n",
    "    output: torch.Tensor\n",
    "        Prediction of the model\n",
    "    \n",
    "    target: torch.Tensor\n",
    "        True label of the predictions\n",
    "    \"\"\"\n",
    "    def __call__(self, model: nn.Module, output: torch.Tensor, target: torch.Tensor):\n",
    "        out = 0\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                out += torch.norm(param, 1)\n",
    "                \n",
    "        return nn.MSELoss(reduction=self.reduction)(output, target) + self.l1*out\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the neural network of example 1 of the paper \n",
    "    \"Learning representations by back-propagating errors\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(6, 2)\n",
    "        self.output = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        sigmoid = torch.sigmoid\n",
    "        out = sigmoid(self.hidden(X))\n",
    "        \n",
    "        return sigmoid(self.output(out))\n",
    "    \n",
    "net = MLP()\n",
    "target = torch.Tensor([1,2,3])\n",
    "out = net(torch.randn((3, 6))).ravel()\n",
    "L1_loss()(net, out, target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
