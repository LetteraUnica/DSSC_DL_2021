{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "devoted-environment",
   "metadata": {},
   "source": [
    "### Deep Learning Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-semiconductor",
   "metadata": {},
   "source": [
    "1. Taking inspiration from the notebook `01-intro-to-pt.ipynb`, build a class for the Multilayer Perceptron (MLP) whose scheme is drawn in the last figure of the notebook. As written there, no layer should have bias units and the activation for each hidden layer should be the Rectified Linear Unit (ReLU) function, also called ramp function. The activation leading to the output layer, instead, should be the softmax function, which prof. Ansuini explained during the last lecture. You can find some notions on it also on the notebook.\n",
    "\n",
    "2. After having defined the class, create an instance of it and print a summary using a method of your choice.\n",
    "\n",
    "3. Provide detailed calculations (layer-by-layer) on the exact number of parameters in the network.  \n",
    "   3.1. Provide the same calculation in the case that the bias units are present in all layers (except input).\n",
    "\n",
    "4. For each layer within the MLP, calculate the L2 norm and L1 norm of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(5, 11, bias=False)\n",
    "        self.layer2 = nn.Linear(11, 16, bias=False)\n",
    "        self.layer3 = nn.Linear(16, 13, bias=False)\n",
    "        self.layer4 = nn.Linear(13, 8, bias=False)\n",
    "        self.output = nn.Linear(8, 4, bias=False)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        relu = nn.functional.relu\n",
    "        softmax = nn.functional.softmax\n",
    "        \n",
    "        out = relu(self.layer1(X))\n",
    "        out = relu(self.layer2(out))\n",
    "        out = relu(self.layer3(out))\n",
    "        out = relu(self.layer4(out))\n",
    "        out = softmax(self.output(out), dim=1)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dress-automation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            55\n",
      "├─Linear: 1-2                            176\n",
      "├─Linear: 1-3                            208\n",
      "├─Linear: 1-4                            104\n",
      "├─Linear: 1-5                            32\n",
      "=================================================================\n",
      "Total params: 575\n",
      "Trainable params: 575\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Linear: 1-1                            55\n",
       "├─Linear: 1-2                            176\n",
       "├─Linear: 1-3                            208\n",
       "├─Linear: 1-4                            104\n",
       "├─Linear: 1-5                            32\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.\n",
    "\n",
    "# Instantiate and test\n",
    "mlp = MLP()\n",
    "X = torch.randn((3, 5))\n",
    "mlp(X)\n",
    "\n",
    "# Summary of the model\n",
    "summary(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-audio",
   "metadata": {},
   "source": [
    "3. In fully connected layers each neuron of layer $i$ is connected to each neuron of layer $i+1$, so if we define $n_i$ as the number of neurons of layer $i$ each neuron of the this layer will have $n_{i+1}$ connections and since we have $n_i$ neurons this will result in a $n_i \\cdot n_{i+1}$ total number of connections, finally each connections represents a parameter so we will have $n_i \\cdot n_{i+1}$ parameters in layer $i+1$. In our network the number of parameters will be:  \n",
    "* layer1: 5*11 = 55\n",
    "* layer2: 11*16 = 176\n",
    "* layer3: 16*13 = 208\n",
    "* layer4: 13*8 = 104\n",
    "* output layer : 8*4 = 32\n",
    "\n",
    "Which corresponds to the summary of our model\n",
    "\n",
    "3.1. In case we have a bias the number of parameters will increase, in particular we will have one additional parameter for each neuron in the $n_{i+1}$ layer, which corresponds to $n_i \\cdot n_{i+1} + n_{i+1} = (n_i+1) \\cdot n_{i+1}$ parameters, this is the same as adding an additional neuron in the $n_i$ layer with value 1. In our network this will correspond to:\n",
    "\n",
    "* layer1: (5+1)*11 = 66\n",
    "* layer2: (11+1)*16 = 192\n",
    "* layer3: (16+1)*13 = 221\n",
    "* layer4: (13+1)*8 = 112\n",
    "* output layer: (8+1)*4 = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blond-canon",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ec81609545f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L2 norm:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L1 norm:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp' is not defined"
     ]
    }
   ],
   "source": [
    "# 4.\n",
    "\n",
    "for param_name, param in mlp.state_dict().items():\n",
    "    print(param_name, \"L2 norm:\", torch.norm(param, 2), \"L1 norm:\", torch.norm(param, 1), param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
